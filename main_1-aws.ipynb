{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sakshamhooda/PneumoniaDetection/blob/main/main_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\">\n",
    "  <img src=\"https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png\" alt=\"Powered by AWS SageMaker\" style=\"height: 40px;\">\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*work was shifted to aws sagemaker due to computation and kernel stability limitations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhdK1l6KfEtc",
    "tags": []
   },
   "source": [
    "# Setting up kaggle for AWS Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (24.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (75.1.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.43.0)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached wheel-0.44.0-py3-none-any.whl (67 kB)\n",
      "Installing collected packages: wheel\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.43.0\n",
      "    Uninstalling wheel-0.43.0:\n",
      "      Successfully uninstalled wheel-0.43.0\n",
      "Successfully installed wheel-0.44.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Tv-J0KXofEWX",
    "outputId": "e978ab36-9a6d-4a3a-d1d2-c6cb770ccfd4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Using cached kaggle-1.6.17.tar.gz (82 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[56 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Usage of dash-separated 'description-file' will not be supported in future\n",
      "  \u001b[31m   \u001b[0m         versions. Please use the underscore name 'description_file' instead.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   opt = self.warn_dash_deprecation(opt, section)\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m creating /tmp/pip-pip-egg-info-6z67n_q7/kaggle.egg-info\n",
      "  \u001b[31m   \u001b[0m writing /tmp/pip-pip-egg-info-6z67n_q7/kaggle.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to /tmp/pip-pip-egg-info-6z67n_q7/kaggle.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing entry points to /tmp/pip-pip-egg-info-6z67n_q7/kaggle.egg-info/entry_points.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to /tmp/pip-pip-egg-info-6z67n_q7/kaggle.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to /tmp/pip-pip-egg-info-6z67n_q7/kaggle.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m writing manifest file '/tmp/pip-pip-egg-info-6z67n_q7/kaggle.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest file '/tmp/pip-pip-egg-info-6z67n_q7/kaggle.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching 'LICENSE.txt'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-b848be4d/kaggle_cc9323c296a34352a5b1c5299b0686f8/setup.py\", line 7, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/__init__.py\", line 117, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 183, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 199, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 954, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 311, in run\n",
      "  \u001b[31m   \u001b[0m     self.find_sources()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 319, in find_sources\n",
      "  \u001b[31m   \u001b[0m     mm.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 545, in run\n",
      "  \u001b[31m   \u001b[0m     self.prune_file_list()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/command/sdist.py\", line 161, in prune_file_list\n",
      "  \u001b[31m   \u001b[0m     super().prune_file_list()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/_distutils/command/sdist.py\", line 380, in prune_file_list\n",
      "  \u001b[31m   \u001b[0m     base_dir = self.distribution.get_fullname()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/_core_metadata.py\", line 267, in get_fullname\n",
      "  \u001b[31m   \u001b[0m     return _distribution_fullname(self.get_name(), self.get_version())\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/setuptools/_core_metadata.py\", line 285, in _distribution_fullname\n",
      "  \u001b[31m   \u001b[0m     canonicalize_version(version, strip_trailing_zero=False),\n",
      "  \u001b[31m   \u001b[0m TypeError: canonicalize_version() got an unexpected keyword argument 'strip_trailing_zero'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hmv: cannot stat ‘kaggle.json’: No such file or directory\n",
      "/bin/sh: kaggle: command not found\n",
      "/bin/sh: kaggle: command not found\n",
      "unzip:  cannot find or open 123-of-ai-presents-pneumonia-detection-from-xray.zip, 123-of-ai-presents-pneumonia-detection-from-xray.zip.zip or 123-of-ai-presents-pneumonia-detection-from-xray.zip.ZIP.\n",
      "ls: cannot access /home/ec2-user/PneumoniaDetection/data/: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install Kaggle API\n",
    "!pip install kaggle\n",
    "\n",
    "# Step 3: Move kaggle.json to the correct folder\n",
    "import os\n",
    "os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "\n",
    "# Move the kaggle.json file to the ~/.kaggle/ folder (replace with the correct path if not in the root)\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "\n",
    "# Step 4: Set permissions for kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Step 5: Verify Kaggle API setup\n",
    "!kaggle competitions list\n",
    "\n",
    "# Step 6: Accept competition rules manually on the Kaggle website\n",
    "\n",
    "# Step 7: Download the competition data\n",
    "!kaggle competitions download -c 123-of-ai-presents-pneumonia-detection-from-xray\n",
    "\n",
    "# Step 8: Unzip the data to a specific directory\n",
    "!unzip 123-of-ai-presents-pneumonia-detection-from-xray.zip -d ~/PneumoniaDetection/data/\n",
    "\n",
    "# Step 9: Verify the files\n",
    "!ls ~/PneumoniaDetection/data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files successfully extracted to data/\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "zip_file_path = \"123-of-ai-presents-pneumonia-detection-from-xray.zip\"\n",
    "extract_dir = \"data/\"\n",
    "\n",
    "# Create the extract directory if it doesn't exist\n",
    "if not os.path.exists(extract_dir):\n",
    "    os.makedirs(extract_dir)\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"Files successfully extracted to {extract_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7UP-eAjiQgH"
   },
   "source": [
    "# Approach\n",
    "\n",
    "1. Understanding the Dataset\n",
    "\n",
    "    Directories and Files:\n",
    "\n",
    "    processed_train_set/: Contains the training X-ray images.\n",
    "    processed_test_set/: Contains the test X-ray images.\n",
    "    train_metadata.csv: Contains mappings of image names to their classes (healthy or pneumonia).\n",
    "    test_files.csv: Contains the list of test image names.\n",
    "    sample_submission.csv: A sample submission file.\n",
    "    Data Columns:\n",
    "\n",
    "    path: The image file name.\n",
    "    class: The ground truth label (healthy or pneumonia).\n",
    "\n",
    "2. Data Preparation\n",
    "\n",
    "    Load train_metadata.csv and test_files.csv.\n",
    "    Ensure that the image paths and labels are correctly mapped.\n",
    "    Use appropriate data generators that match the dataset structure.\n",
    "\n",
    "3. Model Selection\n",
    "\n",
    "    Use Inception V3 as the base model with ImageNet weights.\n",
    "    If desired, ensemble with EfficientNetB0 for improved performance.\n",
    "\n",
    "4. Training Strategy\n",
    "\n",
    "    Freeze the base model layers initially and train the top layers.\n",
    "    Unfreeze some layers for fine-tuning.\n",
    "    Use data augmentation to prevent overfitting.\n",
    "    Monitor the F1 score, as per the competition metric.\n",
    "5. Evaluation and Submission\n",
    "\n",
    "    Evaluate the model on a validation set.\n",
    "    Generate predictions on the test set.\n",
    "    Prepare the submission file matching the required format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ITRXF4_fd4JL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "eCD_deSDeAZE",
    "outputId": "bfcf9278-1210-4c7c-b2ad-66169476d5a7"
   },
   "outputs": [],
   "source": [
    "# Load the dataframes\n",
    "train_df = pd.read_csv('data/1. train_metadata.csv')\n",
    "test_df = pd.read_csv('data/2. test_files.csv')\n",
    "\n",
    "# Display class distribution\n",
    "train_df['class'].value_counts().plot(kind='bar', title='Class Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCi3Lpvziskt"
   },
   "outputs": [],
   "source": [
    "# Add full path to image files\n",
    "train_df['path'] = 'data/processed_test_set/' + train_df['path']\n",
    "test_df['path'] = 'data/processed_test_set/' + test_df['path']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIqg4D9YkJib",
    "outputId": "c3c0e671-e4ef-4db9-fcb0-8ee7960d134f"
   },
   "outputs": [],
   "source": [
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtyEQ1mdkLZT",
    "outputId": "63d5c0ea-6d1c-4470-aac5-5af8400d9592"
   },
   "outputs": [],
   "source": [
    "missing_files = train_df[~train_df['path'].apply(os.path.exists)]\n",
    "print(f\"Missing files:\\n{missing_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8TflzNLjQoM",
    "outputId": "c658cc38-48bc-4e51-f0fd-337230ac99ff"
   },
   "outputs": [],
   "source": [
    "# Define the image size and batch size\n",
    "IMAGE_SIZE = (299, 299)  # InceptionV3 default size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Training data generator with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    #preprocessing_function=preprocess_input,\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    validation_split=0.2  # 20% for validation\n",
    ")\n",
    "\n",
    "# Test data generator\n",
    "test_datagen = ImageDataGenerator(\n",
    "    #preprocessing_function=preprocess_input,\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# Create training and validation generators\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='path',\n",
    "    y_col='class',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='path',\n",
    "    y_col='class',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Create test generator\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='path',\n",
    "    y_col=None,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Bias check\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0fOlLFdjSjU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\n",
    "# Load InceptionV3 with pre-trained ImageNet weights\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(*IMAGE_SIZE, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V95plMWMjVxz"
   },
   "outputs": [],
   "source": [
    "# Add global average pooling and output layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Define the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCOfWcEijX_6"
   },
   "outputs": [],
   "source": [
    "# Freeze all layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5X_1QxAjZzj"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeG8AhWvjbdT"
   },
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Save the best model\n",
    "checkpoint = ModelCheckpoint('inception_v3_best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.2, min_lr=1e-7)\n",
    "\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#custom callback class for monitoring F1 score during training\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_generator):\n",
    "        super().__init__()\n",
    "        self.validation_generator = validation_generator\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.validation_generator.reset()\n",
    "        val_preds = self.model.predict(self.validation_generator)\n",
    "        val_preds = (val_preds > 0.5).astype(int).reshape(-1)\n",
    "        val_f1 = f1_score(self.validation_generator.classes, val_preds)\n",
    "        print(f' - val_f1: {val_f1:.4f}')\n",
    " \n",
    "callbacks.append(F1ScoreCallback(validation_generator))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_Tdbb6zjdEL",
    "outputId": "b943b6f0-a41c-4352-881e-5dbf865c47eb"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    class_weight=class_weights,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Gu_JAPYm-cJ"
   },
   "source": [
    "**Fine-Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8nt_LNb2jmkz"
   },
   "outputs": [],
   "source": [
    "# Unfreeze the top 50 layers of the model\n",
    "for layer in base_model.layers[-50:]:\n",
    "    layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "w7ozHZhhnIav"
   },
   "outputs": [],
   "source": [
    "# Re-compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "q_kxJLi4nJ9U",
    "outputId": "66ee182e-2ab7-41af-b778-80851789ef79"
   },
   "outputs": [],
   "source": [
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-rUyCuUnMku"
   },
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "d4kUw4R0nLYK",
    "outputId": "b16dceff-32c3-40f3-9a3d-1d003a00b40b"
   },
   "outputs": [],
   "source": [
    "# Reset validation generator and get predictions\n",
    "validation_generator.reset()\n",
    "val_preds = model.predict(validation_generator, steps=validation_generator.samples // BATCH_SIZE + 1)\n",
    "val_preds = (val_preds > 0.5).astype(int).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "n194tJqFnQoM"
   },
   "outputs": [],
   "source": [
    "# True labels\n",
    "val_labels = validation_generator.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AcHE7p0knR4i",
    "outputId": "528ade69-c476-4050-b35d-3dc096277279"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1 score\n",
    "val_f1 = f1_score(val_labels, val_preds)\n",
    "print('Validation F1 Score:', val_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW00DrGunVcj"
   },
   "source": [
    "**Generate Predictions on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nNlCzezWnTlJ",
    "outputId": "3f7331ee-34b6-4ede-ad30-280f74f052b5"
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "test_generator.reset()\n",
    "test_preds = model.predict(test_generator, steps=test_generator.samples // BATCH_SIZE + 1)\n",
    "test_preds = (test_preds > 0.5).astype(int).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "RQP_cNM4nahj"
   },
   "outputs": [],
   "source": [
    "# Get the mapping from class indices to labels\n",
    "class_indices = train_generator.class_indices\n",
    "reverse_class_indices = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "# Map predictions to class names\n",
    "test_labels = [reverse_class_indices[pred] for pred in test_preds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GMCBDOgMncnZ"
   },
   "outputs": [],
   "source": [
    "# Prepare submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df.index,\n",
    "    'class': test_labels\n",
    "})\n",
    "\n",
    "# Ensure it matches the sample submission format\n",
    "submission = submission[['ID', 'class']]\n",
    "#submission.columns = ['path', 'class']\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble with EfficientNetBo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXf-iM9fnfyR"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "# Load EfficientNetB0\n",
    "effnet_base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(*IMAGE_SIZE, 3))\n",
    "\n",
    "# Add custom layers\n",
    "x = effnet_base.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Define the model\n",
    "effnet_model = Model(inputs=effnet_base.input, outputs=predictions)\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in effnet_base.layers:\n",
    "    layer.trainable = False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    setting rest part as it is  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Save the best model\n",
    "checkpoint = ModelCheckpoint('inception_v3_best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.2, min_lr=1e-7)\n",
    "\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callback class for monitoring F1 score during training\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_generator):\n",
    "        super().__init__()\n",
    "        self.validation_generator = validation_generator\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.validation_generator.reset()\n",
    "        val_preds = self.model.predict(self.validation_generator)\n",
    "        val_preds = (val_preds > 0.5).astype(int).reshape(-1)\n",
    "        val_f1 = f1_score(self.validation_generator.classes, val_preds)\n",
    "        print(f' - val_f1: {val_f1:.4f}')\n",
    " \n",
    "callbacks.append(F1ScoreCallback(validation_generator))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    class_weight=class_weights,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the top 50 layers of the model\n",
    "for layer in base_model.layers[-50:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Re-compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset validation generator and get predictions\n",
    "validation_generator.reset()\n",
    "val_preds = model.predict(validation_generator, steps=validation_generator.samples // BATCH_SIZE + 1)\n",
    "val_preds = (val_preds > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# True labels\n",
    "val_labels = validation_generator.classes\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1 score\n",
    "val_f1 = f1_score(val_labels, val_preds)\n",
    "print('Validation F1 Score:', val_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions from Inception V3\n",
    "test_generator.reset()\n",
    "inception_preds = model.predict(test_generator, steps=test_generator.samples // BATCH_SIZE + 1)\n",
    "\n",
    "# Predictions from EfficientNetB0\n",
    "test_generator.reset()\n",
    "effnet_preds = effnet_model.predict(test_generator, steps=test_generator.samples // BATCH_SIZE + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the predictions\n",
    "ensemble_preds = (inception_preds + effnet_preds) / 2\n",
    "ensemble_preds = (ensemble_preds > 0.5).astype(int).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map predictions to class names\n",
    "ensemble_labels = [reverse_class_indices[pred] for pred in ensemble_preds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df.index,\n",
    "    'class': test_labels\n",
    "})\n",
    "\n",
    "# Ensure it matches the sample submission format\n",
    "submission = submission[['ID', 'class']]\n",
    "#submission.columns = ['path', 'class']\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPeUny9kuJzVZ6pGwUiOy30",
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
